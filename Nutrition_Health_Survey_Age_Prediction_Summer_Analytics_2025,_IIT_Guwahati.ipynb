{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from scipy import stats\n",
        "from scipy.optimize import minimize_scalar\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import joblib\n",
        "\n",
        "# Core ML Libraries\n",
        "from sklearn.model_selection import (StratifiedKFold, cross_val_score,\n",
        "                                   train_test_split, RepeatedStratifiedKFold)\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import KNNImputer, IterativeImputer\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Imbalanced Learning\n",
        "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
        "from imblearn.under_sampling import EditedNearestNeighbours\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "\n",
        "# Advanced Models\n",
        "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
        "                             GradientBoostingClassifier, VotingClassifier,\n",
        "                             StackingClassifier, BaggingClassifier)\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "# Optimization\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Integer, Categorical\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pOd4UBEAxzk",
        "outputId": "ec3d63d0-9891-4d61-c2e6-4a2a21436da1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All libraries imported successfully\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90c0352f",
        "outputId": "1e9c6e0c-33f7-4eae-b9bc-a4d07604f48b"
      },
      "source": [
        "%pip install catboost"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (1.2.8)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd87d863",
        "outputId": "7a974e8b-b204-4cb5-e873-48612999849b"
      },
      "source": [
        "%pip install scikit-optimize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.11/dist-packages (0.10.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.5.1)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (25.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.6.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (24.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üìä STEP 1: Ultra-Advanced Data Loading & Analysis\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "def comprehensive_data_analysis(train_df, test_df):\n",
        "    \"\"\"Perform deep data analysis for optimal preprocessing strategy\"\"\"\n",
        "\n",
        "    print(f\"üìã Training Data: {train_df.shape[0]} rows, {train_df.shape[1]} columns\")\n",
        "    print(f\"üìã Test Data: {test_df.shape[0]} rows, {test_df.shape[1]} columns\")\n",
        "\n",
        "    # Target distribution analysis\n",
        "    # Check if 'age_group' is in train_df before accessing it\n",
        "    if 'age_group' in train_df.columns:\n",
        "        target_dist = train_df['age_group'].value_counts()\n",
        "        imbalance_ratio = target_dist.min() / target_dist.max()\n",
        "        print(f\"\\nüéØ Target Distribution:\")\n",
        "        # Use .get() with default 0 in case a category is missing after splitting\n",
        "        print(f\"   Adult (0): {target_dist.get(0, 0)} ({target_dist.get(0, 0)/len(train_df)*100:.1f}%)\")\n",
        "        print(f\"   Senior (1): {target_dist.get(1, 0)} ({target_dist.get(1, 0)/len(train_df)*100:.1f}%)\")\n",
        "        print(f\"   Imbalance Ratio: {imbalance_ratio:.3f}\")\n",
        "    else:\n",
        "        print(\"\\nüéØ Target Distribution: 'age_group' column not found in training data for analysis.\")\n",
        "        imbalance_ratio = None # Or a suitable default/indicator\n",
        "\n",
        "    # Missing value analysis\n",
        "    missing_train = train_df.isnull().sum()\n",
        "    missing_test = test_df.isnull().sum()\n",
        "\n",
        "    print(f\"\\nüîç Missing Values Analysis:\")\n",
        "    all_cols = set(missing_train.index).union(set(missing_test.index))\n",
        "    for col in all_cols:\n",
        "        if col in missing_train and missing_train[col] > 0 or (col in missing_test and missing_test[col] > 0):\n",
        "            train_pct = (missing_train[col] / len(train_df)) * 100 if col in missing_train else 0\n",
        "            test_pct = (missing_test[col] / len(test_df)) * 100 if col in missing_test else 0\n",
        "            print(f\"   {col}: Train {train_pct:.1f}%, Test {test_pct:.1f}%\")\n",
        "\n",
        "\n",
        "    # Feature correlation with target\n",
        "    correlations = {}\n",
        "    if 'age_group' in train_df.columns:\n",
        "        numeric_cols = train_df.select_dtypes(include=[np.number]).columns.drop('age_group', errors='ignore')\n",
        "        for col in numeric_cols:\n",
        "            if col in train_df.columns:\n",
        "                # Ensure both columns are numeric before calculating correlation\n",
        "                if pd.api.types.is_numeric_dtype(train_df[col]) and pd.api.types.is_numeric_dtype(train_df['age_group']):\n",
        "                     corr = train_df[col].corr(train_df['age_group'])\n",
        "                     correlations[col] = abs(corr)\n",
        "\n",
        "        print(f\"\\nüîó Top Correlations with Target:\")\n",
        "        sorted_corr = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
        "        for col, corr in sorted_corr[:5]:\n",
        "            print(f\"   {col}: {corr:.4f}\")\n",
        "    else:\n",
        "         print(\"\\nüîó Feature Correlation with Target: 'age_group' column not found for correlation analysis.\")\n",
        "\n",
        "\n",
        "    return imbalance_ratio, correlations\n",
        "\n",
        "# Load data\n",
        "try:\n",
        "    train_df_raw = pd.read_csv('Train_Data.csv')\n",
        "    test_df_raw = pd.read_csv('Test_Data.csv')\n",
        "\n",
        "    # Store test IDs\n",
        "    test_ids = test_df_raw['SEQN'].copy()\n",
        "\n",
        "    # Separate target variable from training data\n",
        "    if 'age_group' in train_df_raw.columns:\n",
        "        train_target = train_df_raw['age_group'].copy()\n",
        "        train_features = train_df_raw.drop(['SEQN', 'age_group'], axis=1)\n",
        "    else:\n",
        "        print(\"‚ùå Error: 'age_group' column not found in Train_Data.csv\")\n",
        "        exit()\n",
        "\n",
        "    # Remove identifier from test data\n",
        "    test_features = test_df_raw.drop('SEQN', axis=1)\n",
        "\n",
        "\n",
        "    imbalance_ratio, feature_correlations = comprehensive_data_analysis(train_df_raw, test_df_raw) # Pass raw data for initial analysis\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "    # exit() # Removing exit to allow further code execution if error is non-fatal\n",
        "\n",
        "print(\"\\n‚úÖ Data loading and analysis complete\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGZ2eOdoBt54",
        "outputId": "47742343-963e-4c12-a209-8f1f13bf3abe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä STEP 1: Ultra-Advanced Data Loading & Analysis\n",
            "--------------------------------------------------\n",
            "üìã Training Data: 1966 rows, 9 columns\n",
            "üìã Test Data: 312 rows, 8 columns\n",
            "\n",
            "üéØ Target Distribution:\n",
            "   Adult (0): 1638 (83.3%)\n",
            "   Senior (1): 314 (16.0%)\n",
            "   Imbalance Ratio: 0.192\n",
            "\n",
            "üîç Missing Values Analysis:\n",
            "   PAQ605: Train 0.7%, Test 0.3%\n",
            "   age_group: Train 0.7%, Test 0.0%\n",
            "   DIQ010: Train 0.9%, Test 0.3%\n",
            "   BMXBMI: Train 0.9%, Test 0.3%\n",
            "   LBXGLU: Train 0.7%, Test 0.3%\n",
            "   LBXGLT: Train 0.6%, Test 0.6%\n",
            "   LBXIN: Train 0.5%, Test 0.3%\n",
            "   SEQN: Train 0.6%, Test 0.6%\n",
            "   RIAGENDR: Train 0.9%, Test 0.6%\n",
            "\n",
            "üîó Top Correlations with Target:\n",
            "\n",
            "‚úÖ Data loading and analysis complete\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üß¨ STEP 2: Medical Domain Expert Feature Engineering\")\n",
        "print(\"-\" * 50)\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.impute import KNNImputer # Import KNNImputer\n",
        "\n",
        "\n",
        "class MedicalFeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Advanced medical domain feature engineering\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.imputer = KNNImputer(n_neighbors=5) # Use KNNImputer\n",
        "        self.scaler = None\n",
        "        self.numeric_cols = None # Add attribute to store numeric column names\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Identify numeric columns\n",
        "        self.numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        # Ensure only numeric columns are selected and convert to numeric, coerce errors to NaN\n",
        "        X_numeric = X[self.numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "        # Fit imputer only on the cleaned numeric columns\n",
        "        self.imputer.fit(X_numeric)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # 1. Handle missing values with imputation\n",
        "        if self.numeric_cols is None:\n",
        "             raise RuntimeError(\"Call fit before calling transform.\")\n",
        "\n",
        "        # Ensure only numeric columns are selected for transformation and convert, coerce errors\n",
        "        X_numeric = X[self.numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "        # print(\"\\n--- Diagnostic: Data before Imputation ---\")\n",
        "        # display(X_numeric.describe(include='all'))\n",
        "        # print(\"----------------------------------------\\n\")\n",
        "\n",
        "\n",
        "        X[self.numeric_cols] = self.imputer.transform(X_numeric)\n",
        "\n",
        "\n",
        "        # 2. MEDICAL EXPERT FEATURES\n",
        "        print(\"üî¨ Creating medical expert features...\")\n",
        "\n",
        "        # Ensure necessary columns exist before creating features\n",
        "        required_medical_cols = ['LBXIN', 'LBXGLU', 'BMXBMI', 'LBXGLT', 'DIQ010', 'RIAGENDR', 'PAQ605']\n",
        "        if not all(col in X.columns for col in required_medical_cols):\n",
        "            print(\"Warning: Not all required medical columns present for feature creation.\")\n",
        "            # You might want to handle this more robustly depending on your needs,\n",
        "            # e.g., by raising an error or skipping feature creation.\n",
        "            # For now, we'll proceed with available columns.\n",
        "\n",
        "        # Insulin Resistance Indices (Check if required cols exist)\n",
        "        if all(col in X.columns for col in ['LBXIN', 'LBXGLU']):\n",
        "            X['HOMA_IR'] = (X['LBXIN'] * X['LBXGLU']) / 405.0\n",
        "            X['QUICKI'] = 1 / (np.log(X['LBXIN'] + 1e-6) + np.log(X['LBXGLU'] + 1e-6))\n",
        "            X['McAuley_Index'] = np.exp(2.63 - 0.28 * np.log(X['LBXIN'] + 1e-6) - 0.31 * np.log(X['LBXGLU'] + 1e-6))\n",
        "\n",
        "        # Metabolic Syndrome Components (Check if required cols exist)\n",
        "        if all(col in X.columns for col in ['BMXBMI', 'LBXGLU', 'LBXGLT', 'DIQ010']):\n",
        "            X['MetSyn_BMI'] = (X['BMXBMI'] >= 30).astype(int)\n",
        "            X['MetSyn_Glucose'] = (X['LBXGLU'] >= 100).astype(int)\n",
        "            X['MetSyn_GTT'] = (X['LBXGLT'] >= 140).astype(int)\n",
        "            X['MetSyn_Score'] = X['MetSyn_BMI'] + X['MetSyn_Glucose'] + X['MetSyn_GTT'] + X['DIQ010']\n",
        "\n",
        "        # Advanced Glucose Metabolism (Check if required cols exist)\n",
        "        if all(col in X.columns for col in ['LBXGLU', 'LBXIN', 'LBXGLT']):\n",
        "            X['Glucose_Insulin_Ratio'] = X['LBXGLU'] / (X['LBXIN'] + 1e-6)\n",
        "            X['GTT_Fasting_Ratio'] = X['LBXGLT'] / (X['LBXGLU'] + 1e-6)\n",
        "            X['Insulin_GTT_Interaction'] = X['LBXIN'] * X['LBXGLT']\n",
        "\n",
        "\n",
        "        # BMI-related Features (Check if required cols exist)\n",
        "        if all(col in X.columns for col in ['BMXBMI', 'LBXGLU', 'LBXIN']):\n",
        "             X['BMI_Category'] = pd.cut(X['BMXBMI'],\n",
        "                                  bins=[0, 18.5, 25, 30, 35, 100],\n",
        "                                  labels=[0, 1, 2, 3, 4],\n",
        "                                  right=True, # Include the right edge of the bin\n",
        "                                  duplicates='drop' # Drop duplicate bin edges if any\n",
        "                                 ).astype('Int64') # Use nullable integer type\n",
        "\n",
        "             X['BMI_Glucose_Product'] = X['BMXBMI'] * X['LBXGLU']\n",
        "             X['BMI_Insulin_Product'] = X['BMXBMI'] * X['LBXIN']\n",
        "\n",
        "\n",
        "        # 3. STATISTICAL TRANSFORMATIONS\n",
        "        print(\"üìä Creating statistical features...\")\n",
        "\n",
        "        # Log transformations for skewed features (Check if required cols exist)\n",
        "        skewed_features = ['LBXIN', 'LBXGLT']\n",
        "        for feat in skewed_features:\n",
        "            if feat in X.columns:\n",
        "                X[f'{feat}_log'] = np.log1p(X[feat])\n",
        "                X[f'{feat}_sqrt'] = np.sqrt(X[feat] + 1e-6)\n",
        "\n",
        "        # Polynomial features for key variables (Check if required cols exist)\n",
        "        key_features = ['BMXBMI', 'LBXGLU', 'LBXIN']\n",
        "        for feat in key_features:\n",
        "            if feat in X.columns:\n",
        "                X[f'{feat}_squared'] = X[feat] ** 2\n",
        "                X[f'{feat}_cubed'] = X[feat] ** 3\n",
        "\n",
        "        # 4. INTERACTION FEATURES\n",
        "        print(\"üîÑ Creating interaction features...\")\n",
        "\n",
        "        # All pairwise interactions of key features (Check if required cols exist)\n",
        "        interaction_features = ['BMXBMI', 'LBXGLU', 'LBXIN', 'LBXGLT']\n",
        "        for i, feat1 in enumerate(interaction_features):\n",
        "            for feat2 in interaction_features[i+1:]:\n",
        "                if feat1 in X.columns and feat2 in X.columns:\n",
        "                    X[f'{feat1}_x_{feat2}'] = X[feat1] * X[feat2]\n",
        "                    X[f'{feat1}_div_{feat2}'] = X[feat1] / (X[feat2] + 1e-6)\n",
        "\n",
        "        # Gender interactions (Check if required cols exist)\n",
        "        if 'RIAGENDR' in X.columns:\n",
        "            for feat in ['BMXBMI', 'LBXGLU', 'LBXIN']:\n",
        "                if feat in X.columns:\n",
        "                    X[f'{feat}_x_Gender'] = X[feat] * X['RIAGENDR']\n",
        "\n",
        "        # Activity interactions (Check if required cols exist)\n",
        "        if 'PAQ605' in X.columns:\n",
        "            for feat in ['BMXBMI', 'LBXGLU']:\n",
        "                if feat in X.columns:\n",
        "                    X[f'{feat}_x_Activity'] = X[feat] * X['PAQ605']\n",
        "\n",
        "        # 5. CLUSTERING FEATURES\n",
        "        print(\"üéØ Creating clustering features...\")\n",
        "\n",
        "        # Create metabolic profiles using clustering (Check if required cols exist)\n",
        "        metabolic_features = ['BMXBMI', 'LBXGLU', 'LBXIN', 'LBXGLT']\n",
        "        if all(feat in X.columns for feat in metabolic_features):\n",
        "            kmeans = KMeans(n_clusters=5, random_state=42, n_init=10) # Add n_init to suppress warning\n",
        "            X['Metabolic_Cluster'] = kmeans.fit_predict(X[metabolic_features])\n",
        "\n",
        "\n",
        "        # 6. PERCENTILE FEATURES\n",
        "        print(\"üìà Creating percentile features...\")\n",
        "\n",
        "        # Convert continuous features to percentile ranks (Check if required cols exist)\n",
        "        for feat in ['BMXBMI', 'LBXGLU', 'LBXIN', 'LBXGLT']:\n",
        "            if feat in X.columns:\n",
        "                X[f'{feat}_percentile'] = stats.rankdata(X[feat]) / len(X)\n",
        "\n",
        "        print(f\"‚úÖ Feature engineering complete. Features: {len(X.columns)}\")\n",
        "        return X\n",
        "\n",
        "# Apply feature engineering\n",
        "feature_engineer = MedicalFeatureEngineer()\n",
        "print(\"üîß Applying to training data...\")\n",
        "train_enhanced = feature_engineer.fit_transform(train_features)\n",
        "\n",
        "print(\"üîß Applying to test data...\")\n",
        "test_enhanced = feature_engineer.transform(test_features)\n",
        "\n",
        "print(f\"üìä Enhanced features: {len(train_enhanced.columns)} (was {len(train_features.columns)})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4W53DZjJClqT",
        "outputId": "64bf7b38-760f-41ce-d49a-5023b19363a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß¨ STEP 2: Medical Domain Expert Feature Engineering\n",
            "--------------------------------------------------\n",
            "üîß Applying to training data...\n",
            "üî¨ Creating medical expert features...\n",
            "üìä Creating statistical features...\n",
            "üîÑ Creating interaction features...\n",
            "üéØ Creating clustering features...\n",
            "üìà Creating percentile features...\n",
            "‚úÖ Feature engineering complete. Features: 52\n",
            "üîß Applying to test data...\n",
            "üî¨ Creating medical expert features...\n",
            "üìä Creating statistical features...\n",
            "üîÑ Creating interaction features...\n",
            "üéØ Creating clustering features...\n",
            "üìà Creating percentile features...\n",
            "‚úÖ Feature engineering complete. Features: 52\n",
            "üìä Enhanced features: 52 (was 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ADVANCED PREPROCESSING PIPELINE\n",
        "# ==============================================================================\n",
        "print(\"\\n‚öôÔ∏è STEP 3: Advanced Preprocessing Pipeline\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "def create_ultra_preprocessing_pipeline(X_train, y_train, X_test):\n",
        "    \"\"\"Ultra-advanced preprocessing for maximum performance\"\"\"\n",
        "\n",
        "    # No need to drop 'age_group' from X_train here, it should already be separated\n",
        "    # if 'age_group' in X_train.columns:\n",
        "    #     X_train = X_train.drop('age_group', axis=1)\n",
        "\n",
        "\n",
        "    print(\"üßπ Advanced outlier detection and removal...\")\n",
        "\n",
        "    # 1. ISOLATION FOREST FOR OUTLIER DETECTION\n",
        "    from sklearn.ensemble import IsolationForest\n",
        "    iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
        "    # Ensure X_train is numeric for IsolationForest\n",
        "    X_train_numeric = X_train.select_dtypes(include=[np.number])\n",
        "    outlier_mask = iso_forest.fit_predict(X_train_numeric) == -1\n",
        "\n",
        "\n",
        "    print(f\"   üéØ Isolation Forest detected {outlier_mask.sum()} outliers\")\n",
        "\n",
        "    # 2. LOCAL OUTLIER FACTOR\n",
        "    from sklearn.neighbors import LocalOutlierFactor\n",
        "    lof = LocalOutlierFactor(contamination=0.05)\n",
        "    # Ensure X_train is numeric for LocalOutlierFactor\n",
        "    lof_outliers = lof.fit_predict(X_train_numeric) == -1\n",
        "\n",
        "\n",
        "    print(f\"   üéØ LOF detected {lof_outliers.sum()} outliers\")\n",
        "\n",
        "    # Combine outlier detection methods\n",
        "    combined_outliers = outlier_mask | lof_outliers\n",
        "    print(f\"   üóëÔ∏è Removing {combined_outliers.sum()} total outliers ({combined_outliers.sum()/len(X_train)*100:.1f}%)\")\n",
        "\n",
        "    X_train_clean = X_train[~combined_outliers]\n",
        "    y_train_clean = y_train[~combined_outliers] # Use y_train for cleaning\n",
        "\n",
        "    # Handle missing values in the target variable after outlier removal\n",
        "    nan_in_target_mask = y_train_clean.isna()\n",
        "    if nan_in_target_mask.sum() > 0:\n",
        "        print(f\"   üóëÔ∏è Removing {nan_in_target_mask.sum()} samples with missing target values after outlier removal.\")\n",
        "        X_train_clean = X_train_clean[~nan_in_target_mask]\n",
        "        y_train_clean = y_train_clean[~nan_in_target_mask]\n",
        "\n",
        "\n",
        "    # Convert target variable to numerical labels\n",
        "    y_train_clean_numeric = y_train_clean.map({'Adult': 0, 'Senior': 1})\n",
        "\n",
        "\n",
        "    # 3. ADVANCED SCALING\n",
        "    print(\"üìè Applying advanced scaling...\")\n",
        "\n",
        "    # Use PowerTransformer for making features more Gaussian\n",
        "    pt = PowerTransformer(method='yeo-johnson')\n",
        "    # Ensure data is numeric before scaling\n",
        "    X_train_numeric_clean = X_train_clean.select_dtypes(include=[np.number])\n",
        "    X_test_numeric = X_test.select_dtypes(include=[np.number])\n",
        "\n",
        "\n",
        "    X_train_transformed = pt.fit_transform(X_train_numeric_clean)\n",
        "    X_test_transformed = pt.transform(X_test_numeric)\n",
        "\n",
        "\n",
        "    # Then apply RobustScaler\n",
        "    scaler = RobustScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_transformed)\n",
        "    X_test_scaled = scaler.transform(X_test_transformed)\n",
        "\n",
        "    # Convert back to DataFrames, preserving original column names\n",
        "    X_train_final = pd.DataFrame(X_train_scaled, columns=X_train_numeric_clean.columns, index=X_train_clean.index)\n",
        "    X_test_final = pd.DataFrame(X_test_scaled, columns=X_test_numeric.columns, index=X_test.index)\n",
        "\n",
        "\n",
        "    # 4. ADVANCED CLASS BALANCING\n",
        "    print(\"‚öñÔ∏è Advanced class balancing...\")\n",
        "\n",
        "    # Ensure only numeric columns are passed to SMOTEENN\n",
        "    X_train_final_numeric = X_train_final.select_dtypes(include=[np.number])\n",
        "\n",
        "    # Use SMOTEENN (SMOTE + Edited Nearest Neighbours)\n",
        "    smoteenn = SMOTEENN(random_state=42)\n",
        "    X_train_balanced, y_train_balanced = smoteenn.fit_resample(X_train_final_numeric, y_train_clean_numeric) # Use y_train_clean_numeric for balancing\n",
        "\n",
        "\n",
        "    print(f\"   üìä Balanced dataset: {len(X_train_balanced)} samples\")\n",
        "    print(f\"   üìä Class distribution: {np.bincount(y_train_balanced)}\")\n",
        "\n",
        "\n",
        "    return X_train_balanced, y_train_balanced, X_test_final, (pt, scaler)\n",
        "\n",
        "# Apply preprocessing\n",
        "X_train_processed, y_train_processed, X_test_processed, preprocessors = create_ultra_preprocessing_pipeline(\n",
        "    train_enhanced, train_target, test_enhanced # Pass train_target as y_train\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Preprocessing complete\\n\")"
      ],
      "metadata": {
        "id": "QNvsKeCRI37T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c22fa00f-1ffc-4584-c56b-b52ba238973d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚öôÔ∏è STEP 3: Advanced Preprocessing Pipeline\n",
            "--------------------------------------------------\n",
            "üßπ Advanced outlier detection and removal...\n",
            "   üéØ Isolation Forest detected 99 outliers\n",
            "   üéØ LOF detected 99 outliers\n",
            "   üóëÔ∏è Removing 141 total outliers (7.2%)\n",
            "   üóëÔ∏è Removing 13 samples with missing target values after outlier removal.\n",
            "üìè Applying advanced scaling...\n",
            "‚öñÔ∏è Advanced class balancing...\n",
            "   üìä Balanced dataset: 2210 samples\n",
            "   üìä Class distribution: [ 904 1306]\n",
            "\n",
            "‚úÖ Preprocessing complete\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INTELLIGENT FEATURE SELECTION\n",
        "# ==============================================================================\n",
        "print(\"\\nüéØ STEP 4: Intelligent Feature Selection\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "def intelligent_feature_selection(X, y, n_features=25):\n",
        "    \"\"\"Multi-method feature selection for optimal performance\"\"\"\n",
        "\n",
        "    print(f\"üîç Selecting top {n_features} features from {X.shape[1]} candidates...\")\n",
        "\n",
        "    # Method 1: Mutual Information\n",
        "    mi_selector = SelectKBest(score_func=mutual_info_classif, k=n_features)\n",
        "    mi_selector.fit(X, y)\n",
        "    mi_features = X.columns[mi_selector.get_support()]\n",
        "\n",
        "    # Method 2: F-statistics\n",
        "    f_selector = SelectKBest(score_func=f_classif, k=n_features)\n",
        "    f_selector.fit(X, y)\n",
        "    f_features = X.columns[f_selector.get_support()]\n",
        "\n",
        "    # Method 3: Random Forest Feature Importance\n",
        "    rf_selector = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf_selector.fit(X, y)\n",
        "    rf_importance = pd.Series(rf_selector.feature_importances_, index=X.columns)\n",
        "    rf_features = rf_importance.nlargest(n_features).index\n",
        "\n",
        "    # Combine selections (features appearing in at least 2 methods)\n",
        "    feature_votes = {}\n",
        "    for feature in X.columns:\n",
        "        votes = 0\n",
        "        if feature in mi_features: votes += 1\n",
        "        if feature in f_features: votes += 1\n",
        "        if feature in rf_features: votes += 1\n",
        "        feature_votes[feature] = votes\n",
        "\n",
        "    # Select features with at least 2 votes, then fill with highest single votes\n",
        "    selected_features = []\n",
        "    for feature, votes in sorted(feature_votes.items(), key=lambda x: x[1], reverse=True):\n",
        "        if len(selected_features) < n_features:\n",
        "            if votes >= 2 or len(selected_features) < n_features//2:\n",
        "                selected_features.append(feature)\n",
        "\n",
        "    print(f\"‚úÖ Selected {len(selected_features)} features using ensemble selection\")\n",
        "    return selected_features\n",
        "\n",
        "# Select best features\n",
        "selected_features = intelligent_feature_selection(X_train_processed, y_train_processed, n_features=30)\n",
        "X_train_selected = X_train_processed[selected_features]\n",
        "X_test_selected = X_test_processed[selected_features]\n",
        "\n",
        "print(f\"üéØ Final feature set: {len(selected_features)} features\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mdc_K5-L8_5",
        "outputId": "4289960a-a4d3-4d21-89ef-623eb28dfac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéØ STEP 4: Intelligent Feature Selection\n",
            "--------------------------------------------------\n",
            "üîç Selecting top 30 features from 52 candidates...\n",
            "‚úÖ Selected 30 features using ensemble selection\n",
            "üéØ Final feature set: 30 features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ULTRA-ADVANCED ENSEMBLE WITH MULTIPLE LEVELS\n",
        "# ==============================================================================\n",
        "print(\"\\nüöÄ STEP 5: Ultra-Advanced Multi-Level Ensemble\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "def create_ultimate_ensemble():\n",
        "    \"\"\"Create the ultimate ensemble for maximum F1 score\"\"\"\n",
        "\n",
        "    print(\"üèóÔ∏è Building multi-level ensemble architecture...\")\n",
        "\n",
        "    # LEVEL 1: Diverse Base Models\n",
        "    base_models = {\n",
        "        'rf_balanced': BalancedRandomForestClassifier(\n",
        "            n_estimators=300,\n",
        "            max_depth=12,\n",
        "            min_samples_split=3,\n",
        "            min_samples_leaf=1,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        ),\n",
        "        'xgb_optimized': xgb.XGBClassifier(\n",
        "            n_estimators=400,\n",
        "            max_depth=7,\n",
        "            learning_rate=0.03,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            reg_alpha=0.1,\n",
        "            reg_lambda=0.1,\n",
        "            scale_pos_weight=3,  # Handle class imbalance\n",
        "            random_state=42,\n",
        "            eval_metric='logloss'\n",
        "        ),\n",
        "        'lgb_optimized': lgb.LGBMClassifier(\n",
        "            n_estimators=400,\n",
        "            max_depth=7,\n",
        "            learning_rate=0.03,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            reg_alpha=0.1,\n",
        "            reg_lambda=0.1,\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            verbose=-1\n",
        "        ),\n",
        "        'catboost': CatBoostClassifier(\n",
        "            iterations=300,\n",
        "            depth=8,\n",
        "            learning_rate=0.05,\n",
        "            l2_leaf_reg=3,\n",
        "            class_weights=[1, 3],  # Handle imbalance\n",
        "            random_seed=42,\n",
        "            verbose=False\n",
        "        ),\n",
        "        'et_balanced': ExtraTreesClassifier(\n",
        "            n_estimators=300,\n",
        "            max_depth=12,\n",
        "            min_samples_split=3,\n",
        "            min_samples_leaf=1,\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        ),\n",
        "        'gb_optimized': GradientBoostingClassifier(\n",
        "            n_estimators=200,\n",
        "            max_depth=6,\n",
        "            learning_rate=0.05,\n",
        "            subsample=0.8,\n",
        "            random_state=42\n",
        "        )\n",
        "    }\n",
        "\n",
        "    # LEVEL 2: Meta-learners with different strengths\n",
        "    meta_models = {\n",
        "        'lr_meta': LogisticRegression(\n",
        "            C=0.1,\n",
        "            class_weight='balanced',\n",
        "            random_state=42\n",
        "        ),\n",
        "        'xgb_meta': xgb.XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=3,\n",
        "            learning_rate=0.1,\n",
        "            scale_pos_weight=3,\n",
        "            random_state=42,\n",
        "            eval_metric='logloss'\n",
        "        )\n",
        "    }\n",
        "\n",
        "    # Create multiple stacking ensembles\n",
        "    ensemble_1 = StackingClassifier(\n",
        "        estimators=list(base_models.items())[:3],\n",
        "        final_estimator=meta_models['lr_meta'],\n",
        "        cv=5,\n",
        "        stack_method='predict_proba',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    ensemble_2 = StackingClassifier(\n",
        "        estimators=list(base_models.items())[3:],\n",
        "        final_estimator=meta_models['xgb_meta'],\n",
        "        cv=5,\n",
        "        stack_method='predict_proba',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # LEVEL 3: Final ensemble of ensembles\n",
        "    final_ensemble = VotingClassifier(\n",
        "        estimators=[\n",
        "            ('stack1', ensemble_1),\n",
        "            ('stack2', ensemble_2)\n",
        "        ],\n",
        "        voting='soft',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    return final_ensemble, base_models\n",
        "\n",
        "# Create the ultimate ensemble\n",
        "print(\"üéØ Creating ultimate ensemble...\")\n",
        "ultimate_ensemble, base_models = create_ultimate_ensemble()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6Vl_SV5MEr7",
        "outputId": "5774e9ad-7744-41af-b294-1ffe9b6e4f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ STEP 5: Ultra-Advanced Multi-Level Ensemble\n",
            "--------------------------------------------------\n",
            "üéØ Creating ultimate ensemble...\n",
            "üèóÔ∏è Building multi-level ensemble architecture...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# THRESHOLD OPTIMIZATION FOR F1 SCORE\n",
        "# ==============================================================================\n",
        "print(\"\\nüéöÔ∏è STEP 6: F1 Score Threshold Optimization\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "def optimize_f1_threshold(model, X, y, cv_folds=5):\n",
        "    \"\"\"Find optimal threshold for maximum F1 score\"\"\"\n",
        "\n",
        "    print(\"üîç Optimizing threshold for F1 score...\")\n",
        "\n",
        "    # Get cross-validation predictions\n",
        "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "    y_proba_cv = np.zeros(len(y))\n",
        "\n",
        "    for train_idx, val_idx in skf.split(X, y):\n",
        "        X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_fold_train = y.iloc[train_idx] if hasattr(y, 'iloc') else y[train_idx]\n",
        "\n",
        "        model.fit(X_fold_train, y_fold_train)\n",
        "        y_proba_cv[val_idx] = model.predict_proba(X_fold_val)[:, 1]\n",
        "\n",
        "    # Find optimal threshold\n",
        "    thresholds = np.arange(0.1, 0.9, 0.01)\n",
        "    best_f1 = 0\n",
        "    best_threshold = 0.5\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        y_pred_thresh = (y_proba_cv >= threshold).astype(int)\n",
        "        f1 = f1_score(y, y_pred_thresh)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_threshold = threshold\n",
        "\n",
        "    print(f\"‚úÖ Optimal threshold: {best_threshold:.3f}\")\n",
        "    print(f\"‚úÖ Optimal F1 score: {best_f1:.4f}\")\n",
        "\n",
        "    return best_threshold, best_f1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NB0fD1PBMOXR",
        "outputId": "70b243e5-8695-496e-e173-3dcba2f34b1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéöÔ∏è STEP 6: F1 Score Threshold Optimization\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPREHENSIVE MODEL EVALUATION\n",
        "# ==============================================================================\n",
        "print(\"\\nüìä STEP 7: Comprehensive Model Evaluation\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "def comprehensive_evaluation(model, X, y, cv_folds=5):\n",
        "    \"\"\"Comprehensive evaluation with multiple metrics\"\"\"\n",
        "\n",
        "    print(\"üìà Running comprehensive evaluation...\")\n",
        "\n",
        "    # Repeated Stratified K-Fold for robust evaluation\n",
        "    rskf = RepeatedStratifiedKFold(n_splits=cv_folds, n_repeats=3, random_state=42)\n",
        "\n",
        "    f1_scores = []\n",
        "    for fold, (train_idx, val_idx) in enumerate(rskf.split(X, y)):\n",
        "        X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_fold_train = y.iloc[train_idx] if hasattr(y, 'iloc') else y[train_idx]\n",
        "        y_fold_val = y.iloc[val_idx] if hasattr(y, 'iloc') else y[val_idx]\n",
        "\n",
        "        model.fit(X_fold_train, y_fold_train)\n",
        "        y_pred = model.predict(X_fold_val)\n",
        "        f1 = f1_score(y_fold_val, y_pred)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "        if fold < 5:  # Print first 5 folds\n",
        "            print(f\"   Fold {fold+1:2d}: F1 = {f1:.4f}\")\n",
        "\n",
        "    mean_f1 = np.mean(f1_scores)\n",
        "    std_f1 = np.std(f1_scores)\n",
        "\n",
        "    print(f\"\\nüéØ Final Results:\")\n",
        "    print(f\"   Mean F1 Score: {mean_f1:.4f} ¬± {std_f1:.4f}\")\n",
        "    print(f\"   Min F1 Score:  {np.min(f1_scores):.4f}\")\n",
        "    print(f\"   Max F1 Score:  {np.max(f1_scores):.4f}\")\n",
        "\n",
        "    return mean_f1, std_f1\n",
        "\n",
        "# Train and evaluate the ultimate ensemble\n",
        "print(\"üöÄ Training ultimate ensemble...\")\n",
        "mean_f1, std_f1 = comprehensive_evaluation(\n",
        "    ultimate_ensemble,\n",
        "    X_train_selected,\n",
        "    pd.Series(y_train_processed) if not isinstance(y_train_processed, pd.Series) else y_train_processed\n",
        ")\n",
        "\n",
        "# Find optimal threshold\n",
        "optimal_threshold, optimal_f1 = optimize_f1_threshold(\n",
        "    ultimate_ensemble,\n",
        "    X_train_selected,\n",
        "    pd.Series(y_train_processed) if not isinstance(y_train_processed, pd.Series) else y_train_processed\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIGhlgWNMUHU",
        "outputId": "3c1b4b49-07e0-422f-9785-956634edac5b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä STEP 7: Comprehensive Model Evaluation\n",
            "--------------------------------------------------\n",
            "üöÄ Training ultimate ensemble...\n",
            "üìà Running comprehensive evaluation...\n",
            "   Fold  1: F1 = 0.9416\n",
            "   Fold  2: F1 = 0.9585\n",
            "   Fold  3: F1 = 0.9699\n",
            "   Fold  4: F1 = 0.9515\n",
            "   Fold  5: F1 = 0.9623\n",
            "\n",
            "üéØ Final Results:\n",
            "   Mean F1 Score: 0.9575 ¬± 0.0095\n",
            "   Min F1 Score:  0.9416\n",
            "   Max F1 Score:  0.9719\n",
            "üîç Optimizing threshold for F1 score...\n",
            "‚úÖ Optimal threshold: 0.690\n",
            "‚úÖ Optimal F1 score: 0.9579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FINAL PREDICTION WITH OPTIMIZED THRESHOLD\n",
        "# ==============================================================================\n",
        "print(\"\\nüéØ STEP 8: Final Prediction Generation\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"üî• Training final model on complete dataset...\")\n",
        "ultimate_ensemble.fit(X_train_selected, y_train_processed)\n",
        "\n",
        "print(\"üîÆ Generating predictions with optimized threshold...\")\n",
        "test_probabilities = ultimate_ensemble.predict_proba(X_test_selected)[:, 1]\n",
        "test_predictions = (test_probabilities >= optimal_threshold).astype(int)\n",
        "\n",
        "# Create submission\n",
        "submission = pd.DataFrame({\n",
        "    'SEQN': test_ids,\n",
        "    'age_group': test_predictions\n",
        "})\n",
        "\n",
        "# Final validation\n",
        "print(f\"\\n‚úÖ SUBMISSION VALIDATION:\")\n",
        "print(f\"   Shape: {submission.shape}\")\n",
        "print(f\"   Unique values: {sorted(submission['age_group'].unique())}\")\n",
        "print(f\"   Distribution: {submission['age_group'].value_counts().to_dict()}\")\n",
        "print(f\"   Missing values: {submission.isnull().sum().sum()}\")\n",
        "\n",
        "# Save submission\n",
        "submission.to_csv('submission.csv', index=False)\n"
      ],
      "metadata": {
        "id": "MTfcde_JSoS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c66385b9-de09-41d9-f8c7-21c9402fb1b5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéØ STEP 8: Final Prediction Generation\n",
            "--------------------------------------------------\n",
            "üî• Training final model on complete dataset...\n",
            "üîÆ Generating predictions with optimized threshold...\n",
            "\n",
            "‚úÖ SUBMISSION VALIDATION:\n",
            "   Shape: (312, 2)\n",
            "   Unique values: [np.int64(0), np.int64(1)]\n",
            "   Distribution: {0: 230, 1: 82}\n",
            "   Missing values: 2\n"
          ]
        }
      ]
    }
  ]
}